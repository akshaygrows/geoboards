{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import psycopg2 as psy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conn(SSH_required,key_path):   #for getting a connection as a result\n",
    "\n",
    "    db='datawarehouse'\n",
    "    DB_HOST='datawarehouse.cdgpvetprks3.ap-south-1.rds.amazonaws.com'\n",
    "    conn = []\n",
    "    if SSH_required == 'Yes':\n",
    "        SSH_HOST='ec2-15-206-161-154.ap-south-1.compute.amazonaws.com'\n",
    "        #LOCALHOST=\"0.0.0.0\"\n",
    "        ssh_tunnel= SSHTunnelForwarder(\n",
    "                (SSH_HOST, 22),\n",
    "                ssh_username=\"ec2-user\",\n",
    "                ssh_private_key= key_path,\n",
    "                ssh_private_key_password= \"\",\n",
    "                remote_bind_address=(DB_HOST, 5432),\n",
    "                local_bind_address=('127.0.0.1', 0)\n",
    "        )\n",
    "        print('Tunnel Started')\n",
    "        ssh_tunnel.start()\n",
    "        conn = psy.connect(\n",
    "            host=ssh_tunnel.local_bind_host,\n",
    "            port=ssh_tunnel.local_bind_port,\n",
    "            user='postgres',\n",
    "            password= \"Simply1234\",\n",
    "            database='postgres')\n",
    "        print('Connection Made')\n",
    "        return conn\n",
    "    else:\n",
    "        conn = psy.connect(\n",
    "            host = DB_HOST,\n",
    "            port = 5432,\n",
    "            user = 'postgres',\n",
    "            password= \"Simply1234\",\n",
    "            database='postgres')\n",
    "        print('Connection Made')\n",
    "        return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_sql(SSH_required, query,key_path):   #for getting a datafarame as a result\n",
    "\n",
    "    db='datawarehouse'\n",
    "    DB_HOST='datawarehouse.cdgpvetprks3.ap-south-1.rds.amazonaws.com'\n",
    "    conn = None\n",
    "    if SSH_required == 'Yes':\n",
    "        SSH_HOST='ec2-15-206-161-154.ap-south-1.compute.amazonaws.com'\n",
    "        #LOCALHOST=\"0.0.0.0\"\n",
    "        ssh_tunnel= SSHTunnelForwarder(\n",
    "                (SSH_HOST, 22),\n",
    "                ssh_username=\"ec2-user\",\n",
    "                ssh_private_key= key_path,\n",
    "                ssh_private_key_password= \"\",\n",
    "                remote_bind_address=(DB_HOST, 5432),\n",
    "                local_bind_address=('127.0.0.1', 0)\n",
    "        )\n",
    "        # ssh_tunnel._server_list[0].block_on_close = False\n",
    "        ssh_tunnel.start()\n",
    "        conn = psy.connect(\n",
    "            host=ssh_tunnel.local_bind_host,\n",
    "            port=ssh_tunnel.local_bind_port,\n",
    "            user='postgres',\n",
    "            password= \"Simply1234\",\n",
    "            database='postgres')\n",
    "        df_results = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        ssh_tunnel.stop()\n",
    "        return df_results\n",
    "    else:\n",
    "        conn = psy.connect(\n",
    "            host = DB_HOST,\n",
    "            port = 5432,\n",
    "            user = 'postgres',\n",
    "            password= \"Simply1234\",\n",
    "            database='postgres')\n",
    "        df_results = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunnel Started\n",
      "Connection Made\n",
      "      warehouse_id          pickuptime           awb\n",
      "0              329 2024-07-08 10:56:17  GS1120176997\n",
      "1              329 2024-07-08 10:56:45  GS1080402563\n",
      "2              329 2024-07-08 10:56:56  GS2023280325\n",
      "3              329 2024-07-08 10:57:07  GS1633410405\n",
      "4              329 2024-07-08 10:57:30  GS1509814115\n",
      "...            ...                 ...           ...\n",
      "7093          2147 2024-07-08 12:00:50  GS1515106055\n",
      "7094          2147 2024-07-08 12:00:57  GS2112840975\n",
      "7095          2147 2024-07-08 12:01:07  GS1835884817\n",
      "7096          2147 2024-07-08 12:01:20  GS1685924606\n",
      "7097          2147 2024-07-08 12:01:34  GS1269768514\n",
      "\n",
      "[7098 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Usage with the actual path to the private key\n",
    "SSH_required = 'Yes'\n",
    "key_path = '/Users/rajatsansaniwal/Documents/tunnel-ssh .cer'\n",
    "query = \"\"\"with ops_main as (\n",
    "    select\n",
    "        awb\n",
    "    ,   pickuptime\n",
    "    from public.ops_main\n",
    "    where 1=1\n",
    "    and pickuptime is not null\n",
    "    and shipping_partner = 'Hyperlocal'\n",
    "    and date_trunc('day', pickuptime) = date_trunc('day', now() + interval'5.5 hours')\n",
    ")\n",
    ",\n",
    "shipment_order_details as (\n",
    "    select\n",
    "        awb\n",
    "    ,   warehouse_id\n",
    "    from public.shipment_order_details\n",
    ")\n",
    ",\n",
    "base as (\n",
    "    select\n",
    "        s.warehouse_id\n",
    "    ,   o.pickuptime\n",
    "    ,   o.awb\n",
    "    from ops_main o\n",
    "    left join shipment_order_details s on o.awb = s.awb\n",
    "    order by 1, 2\n",
    ")\n",
    "\n",
    "select * from base\"\"\"\n",
    "\n",
    "# Establish a connection\n",
    "conn = get_conn(SSH_required, key_path)\n",
    "\n",
    "# Retrieve data into a DataFrame\n",
    "df = get_df_from_sql(SSH_required, query, key_path)\n",
    "\n",
    "# Now you can perform further operations with the DataFrame 'df'\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    warehouse_id pickup_start_time pickup_end_time  slot_duration  gap_to_next\n",
      "0            329             10:56           11:17             22          NaN\n",
      "1            370             03:49           03:58             10          NaN\n",
      "2            403             10:42           11:13             32          NaN\n",
      "3            833             08:10           08:16              6        164.0\n",
      "4            833             10:59           10:59              1          NaN\n",
      "..           ...               ...             ...            ...          ...\n",
      "60          2069             09:21           09:23              2          NaN\n",
      "61          2082             00:07           00:07              0          NaN\n",
      "62          2129             11:58           12:01              3          NaN\n",
      "63          2140             10:18           10:39             21          NaN\n",
      "64          2147             11:58           12:01              3          NaN\n",
      "\n",
      "[65 rows x 5 columns]\n",
      "DataFrame saved to results_df.xlsx\n"
     ]
    }
   ],
   "source": [
    "df['pickuptime'] = pd.to_datetime(df['pickuptime'])\n",
    "\n",
    "results = []\n",
    "\n",
    "for warehouse, group in df.groupby('warehouse_id'):\n",
    "    group = group.reset_index(drop=True)\n",
    "\n",
    "    pickup_start_time = group.loc[0, 'pickuptime']\n",
    "\n",
    "    for i in range(1, len(group)):\n",
    "        current_time = group.loc[i, 'pickuptime']\n",
    "        previous_time = group.loc[i - 1, 'pickuptime']\n",
    "        \n",
    "        # Check the difference between current and previous pickuptime\n",
    "        if (current_time - previous_time).total_seconds() > 1800:  # More than half hour\n",
    "            # End the current pickup slot and start a new one\n",
    "            results.append({\n",
    "                'warehouse_id': warehouse,\n",
    "                'pickup_start_time': pickup_start_time,\n",
    "                'pickup_end_time': previous_time\n",
    "            })\n",
    "            pickup_start_time = current_time  # Update the start time for the new slot\n",
    "    \n",
    "    # Append the last slot for the warehouse\n",
    "    results.append({\n",
    "        'warehouse_id': warehouse,\n",
    "        'pickup_start_time': pickup_start_time,\n",
    "        'pickup_end_time': group.loc[len(group) - 1, 'pickuptime']\n",
    "    })\n",
    "\n",
    "# Create DataFrame from results list\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate slot_duration in minutes\n",
    "results_df['slot_duration'] = ((results_df['pickup_end_time'] - results_df['pickup_start_time']).dt.total_seconds() / 60).apply(math.ceil)\n",
    "\n",
    "# Sort DataFrame by warehouse_id and pickup_start_time (if not already sorted)\n",
    "results_df.sort_values(by=['warehouse_id', 'pickup_start_time'], inplace=True)\n",
    "\n",
    "# Calculate gaps between pickup slots in minutes\n",
    "results_df['gap_to_next'] = results_df.groupby('warehouse_id')['pickup_start_time'].shift(-1) - results_df['pickup_end_time']\n",
    "results_df['gap_to_next'] = (results_df['gap_to_next'].dt.total_seconds() / 60)\n",
    "results_df['gap_to_next'].fillna(0, inplace=True)\n",
    "results_df['gap_to_next'] = (results_df['gap_to_next'].astype(int) + 1).replace(1, np.nan)\n",
    "\n",
    "# Keep only the time portion (hours and minutes) for pickup_start_time and pickup_end_time\n",
    "results_df['pickup_start_time'] = results_df['pickup_start_time'].dt.strftime('%H:%M')\n",
    "results_df['pickup_end_time'] = results_df['pickup_end_time'].dt.strftime('%H:%M')\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "excel_file = 'results_df.xlsx'\n",
    "results_df.to_excel(excel_file, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {excel_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 window  active  warehouse_id  pickup_start  pickup_end\n",
      "515 2024-07-08 10:00:00    True          1615          True       False\n",
      "615 2024-07-08 12:00:00   False          1615         False        True\n"
     ]
    }
   ],
   "source": [
    "df['pickuptime'] = pd.to_datetime(df['pickuptime'])\n",
    "\n",
    "def round_down_to_nearest_10min(dt):\n",
    "    return dt - timedelta(minutes=dt.minute, seconds=dt.second, microseconds=dt.microsecond)\n",
    "\n",
    "df['window'] = df['pickuptime'].apply(round_down_to_nearest_10min)\n",
    "\n",
    "# Generate all possible 10-minute windows\n",
    "start_time = df['pickuptime'].min().floor('H')\n",
    "end_time = df['pickuptime'].max().ceil('H')\n",
    "all_windows = pd.date_range(start=start_time, end=end_time, freq='H')\n",
    "\n",
    "# Create a DataFrame for windows and initialize as inactive\n",
    "window_df = pd.DataFrame({'window': all_windows})\n",
    "window_df['active'] = False\n",
    "\n",
    "# Add warehouse_id to windows and initialize as inactive\n",
    "unique_warehouses = df['warehouse_id'].unique()\n",
    "window_df = window_df.merge(pd.DataFrame({'warehouse_id': unique_warehouses}), how='cross')\n",
    "window_df['active'] = False\n",
    "\n",
    "# Mark active windows based on pickups\n",
    "for warehouse in unique_warehouses:\n",
    "    warehouse_windows = df[df['warehouse_id'] == warehouse]['window'].unique()\n",
    "    window_df.loc[(window_df['warehouse_id'] == warehouse) & (window_df['window'].isin(warehouse_windows)), 'active'] = True\n",
    "\n",
    "# Identify pickup start and end times\n",
    "window_df['pickup_start'] = False\n",
    "window_df['pickup_end'] = False\n",
    "\n",
    "for warehouse in unique_warehouses:\n",
    "    warehouse_windows = window_df[window_df['warehouse_id'] == warehouse].reset_index(drop=True)\n",
    "    for i in range(len(warehouse_windows) - 1):\n",
    "        current_active = warehouse_windows.iloc[i]['active']\n",
    "        next_active = warehouse_windows.iloc[i + 1]['active']\n",
    "        if i == 0 and current_active:\n",
    "            window_df.loc[(window_df['warehouse_id'] == warehouse) & (window_df['window'] == warehouse_windows.iloc[i]['window']), 'pickup_start'] = True\n",
    "        if not current_active and next_active:\n",
    "            window_df.loc[(window_df['warehouse_id'] == warehouse) & (window_df['window'] == warehouse_windows.iloc[i + 1]['window']), 'pickup_start'] = True\n",
    "        if current_active and not next_active:\n",
    "            if i > 0:  # Ensure it's not the first window of the day\n",
    "                window_df.loc[(window_df['warehouse_id'] == warehouse) & (window_df['window'] == warehouse_windows.iloc[i + 1]['window']), 'pickup_end'] = True\n",
    "\n",
    "\n",
    "# Filter to show only start and end times\n",
    "pickup_times = window_df[(window_df['pickup_start']) | (window_df['pickup_end'])]\n",
    "\n",
    "# Display results\n",
    "print(pickup_times[pickup_times['warehouse_id'] == 1615])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m j \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m j \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(warehouse_windows):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mwarehouse_windows\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickup_end\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# Append the result for this pair of pickup_start and pickup_end\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarehouse_id\u001b[39m\u001b[38;5;124m'\u001b[39m: warehouse,\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickup_start_time\u001b[39m\u001b[38;5;124m'\u001b[39m: warehouse_windows\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickup_end_time\u001b[39m\u001b[38;5;124m'\u001b[39m: warehouse_windows\u001b[38;5;241m.\u001b[39miloc[j][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m         })\n\u001b[1;32m     21\u001b[0m         i \u001b[38;5;241m=\u001b[39m j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Move to the next pickup_start\u001b[39;00m\n",
      "File \u001b[0;32m~/git_geoboards/geoboards/.venv/lib/python3.9/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_geoboards/geoboards/.venv/lib/python3.9/site-packages/pandas/core/indexing.py:1754\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[0;32m-> 1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_geoboards/geoboards/.venv/lib/python3.9/site-packages/pandas/core/frame.py:3996\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3994\u001b[0m \u001b[38;5;66;03m# irow\u001b[39;00m\n\u001b[1;32m   3995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3996\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_xs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3998\u001b[0m     \u001b[38;5;66;03m# if we are a copy, mark as such\u001b[39;00m\n\u001b[1;32m   3999\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(new_mgr\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m new_mgr\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git_geoboards/geoboards/.venv/lib/python3.9/site-packages/pandas/core/internals/managers.py:1001\u001b[0m, in \u001b[0;36mBlockManager.fast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    996\u001b[0m     result \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(result)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# Such assignment may incorrectly coerce NaT to None\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;66;03m# result[blk.mgr_locs] = blk._slice((slice(None), loc))\u001b[39;00m\n\u001b[0;32m-> 1001\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, rl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmgr_locs\u001b[49m):\n\u001b[1;32m   1002\u001b[0m         result[rl] \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39miget((i, loc))\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n",
      "File \u001b[0;32m~/git_geoboards/geoboards/.venv/lib/python3.9/site-packages/pandas/core/internals/blocks.py:266\u001b[0m, in \u001b[0;36mBlock.mgr_locs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmgr_locs\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BlockPlacement:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr_locs\n\u001b[1;32m    270\u001b[0m \u001b[38;5;129m@mgr_locs\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmgr_locs\u001b[39m(\u001b[38;5;28mself\u001b[39m, new_mgr_locs: BlockPlacement) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Iterate through each warehouse_id\n",
    "for warehouse in window_df['warehouse_id'].unique():\n",
    "    # Filter rows for the current warehouse_id and reset index for iteration\n",
    "    warehouse_windows = window_df[window_df['warehouse_id'] == warehouse].reset_index(drop=True)\n",
    "    i = 0\n",
    "    while i < len(warehouse_windows):\n",
    "        if warehouse_windows.iloc[i]['pickup_start']:\n",
    "            # Find the corresponding pickup_end\n",
    "            j = i + 1\n",
    "            while j < len(warehouse_windows):\n",
    "                if warehouse_windows.iloc[j]['pickup_end']:\n",
    "                    # Append the result for this pair of pickup_start and pickup_end\n",
    "                    results.append({\n",
    "                        'warehouse_id': warehouse,\n",
    "                        'pickup_start_time': warehouse_windows.iloc[i]['window'],\n",
    "                        'pickup_end_time': warehouse_windows.iloc[j]['window']\n",
    "                    })\n",
    "                    i = j + 1  # Move to the next pickup_start\n",
    "                    break\n",
    "                j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "# Create DataFrame from results list\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Step 1: Calculate slot_duration in minutes\n",
    "results_df['slot_duration'] = (results_df['pickup_end_time'] - results_df['pickup_start_time']).dt.total_seconds() / 60\n",
    "\n",
    "# Sort DataFrame by warehouse_id and pickup_start_time (if not already sorted)\n",
    "results_df.sort_values(by=['warehouse_id', 'pickup_start_time'], inplace=True)\n",
    "\n",
    "# Initialize gap_to_next column with NaN\n",
    "results_df['gap_to_next'] = pd.NaT\n",
    "\n",
    "# Iterate over each unique warehouse_id\n",
    "unique_warehouses = results_df['warehouse_id'].unique()\n",
    "for warehouse_id in unique_warehouses:\n",
    "    # Select rows for the current warehouse_id\n",
    "    warehouse_df = results_df[results_df['warehouse_id'] == warehouse_id].copy()\n",
    "    \n",
    "    # Calculate gaps between pickup slots in minutes\n",
    "    warehouse_df['gap_to_next'] = (warehouse_df['pickup_start_time'].shift(-1) - warehouse_df['pickup_end_time']).dt.total_seconds() / 60\n",
    "    warehouse_df.loc[warehouse_df.index[-1], 'gap_to_next'] = pd.NaT  # Set NaN for the last row\n",
    "    \n",
    "    # Update results_df with calculated gaps for the current warehouse_id\n",
    "    results_df.loc[results_df['warehouse_id'] == warehouse_id, 'gap_to_next'] = warehouse_df['gap_to_next']\n",
    "\n",
    "\n",
    "# Print the result\n",
    "print(results_df)\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "excel_file = 'results_df.xlsx'\n",
    "results_df.to_excel(excel_file, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {excel_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m j \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m j \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(warehouse_windows):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mwarehouse_windows\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickup_end\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# Append the result for this pair of pickup_start and pickup_end\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarehouse_id\u001b[39m\u001b[38;5;124m'\u001b[39m: warehouse,\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickup_start_time\u001b[39m\u001b[38;5;124m'\u001b[39m: warehouse_windows\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickup_end_time\u001b[39m\u001b[38;5;124m'\u001b[39m: warehouse_windows\u001b[38;5;241m.\u001b[39miloc[j][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m         })\n\u001b[1;32m     21\u001b[0m         i \u001b[38;5;241m=\u001b[39m j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Move to the next pickup_start\u001b[39;00m\n",
      "File \u001b[0;32m~/git_geoboards/geoboards/.venv/lib/python3.9/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_geoboards/geoboards/.venv/lib/python3.9/site-packages/pandas/core/indexing.py:1754\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[0;32m-> 1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git_geoboards/geoboards/.venv/lib/python3.9/site-packages/pandas/core/frame.py:4002\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   4000\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced_from_mgr(new_mgr, axes\u001b[38;5;241m=\u001b[39mnew_mgr\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   4001\u001b[0m result\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex[i]\n\u001b[0;32m-> 4002\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__finalize__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4003\u001b[0m result\u001b[38;5;241m.\u001b[39m_set_is_copy(\u001b[38;5;28mself\u001b[39m, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m   4004\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/git_geoboards/geoboards/.venv/lib/python3.9/site-packages/pandas/core/generic.py:6262\u001b[0m, in \u001b[0;36mNDFrame.__finalize__\u001b[0;34m(self, other, method, **kwargs)\u001b[0m\n\u001b[1;32m   6255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[1;32m   6256\u001b[0m     \u001b[38;5;66;03m# We want attrs propagation to have minimal performance\u001b[39;00m\n\u001b[1;32m   6257\u001b[0m     \u001b[38;5;66;03m# impact if attrs are not used; i.e. attrs is an empty dict.\u001b[39;00m\n\u001b[1;32m   6258\u001b[0m     \u001b[38;5;66;03m# One could make the deepcopy unconditionally, but a deepcopy\u001b[39;00m\n\u001b[1;32m   6259\u001b[0m     \u001b[38;5;66;03m# of an empty dict is 50x more expensive than the empty check.\u001b[39;00m\n\u001b[1;32m   6260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs \u001b[38;5;241m=\u001b[39m deepcopy(other\u001b[38;5;241m.\u001b[39mattrs)\n\u001b[0;32m-> 6262\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallows_duplicate_labels\u001b[49m \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mallows_duplicate_labels\n\u001b[1;32m   6263\u001b[0m \u001b[38;5;66;03m# For subclasses using _metadata.\u001b[39;00m\n\u001b[1;32m   6264\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(other\u001b[38;5;241m.\u001b[39m_metadata):\n",
      "File \u001b[0;32m~/git_geoboards/geoboards/.venv/lib/python3.9/site-packages/pandas/core/flags.py:87\u001b[0m, in \u001b[0;36mFlags.allows_duplicate_labels\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    Whether this object allows duplicate labels.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    a        [0, 1]\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allows_duplicate_labels\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;129m@allows_duplicate_labels\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mallows_duplicate_labels\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(value)\n\u001b[1;32m     90\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Group by warehouse_id and iterate over each group\n",
    "for warehouse, warehouse_windows in window_df.groupby('warehouse_id'):\n",
    "    # Reset index for iteration\n",
    "    warehouse_windows = warehouse_windows.reset_index(drop=True)\n",
    "    i = 0\n",
    "    while i < len(warehouse_windows):\n",
    "        if warehouse_windows.iloc[i]['pickup_start']:\n",
    "            # Find the corresponding pickup_end\n",
    "            j = i + 1\n",
    "            while j < len(warehouse_windows):\n",
    "                if warehouse_windows.iloc[j]['pickup_end']:\n",
    "                    # Append the result for this pair of pickup_start and pickup_end\n",
    "                    results.append({\n",
    "                        'warehouse_id': warehouse,\n",
    "                        'pickup_start_time': warehouse_windows.iloc[i]['window'],\n",
    "                        'pickup_end_time': warehouse_windows.iloc[j]['window']\n",
    "                    })\n",
    "                    i = j + 1  # Move to the next pickup_start\n",
    "                    break\n",
    "                j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "# Create DataFrame from results list\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Step 1: Calculate slot_duration in minutes\n",
    "results_df['slot_duration'] = (results_df['pickup_end_time'] - results_df['pickup_start_time']).dt.total_seconds() / 60\n",
    "\n",
    "# Sort DataFrame by warehouse_id and pickup_start_time (if not already sorted)\n",
    "results_df.sort_values(by=['warehouse_id', 'pickup_start_time'], inplace=True)\n",
    "\n",
    "# Calculate gaps between pickup slots in minutes\n",
    "results_df['gap_to_next'] = results_df.groupby('warehouse_id')['pickup_start_time'].shift(-1) - results_df['pickup_end_time']\n",
    "results_df['gap_to_next'] = results_df['gap_to_next'].dt.total_seconds() / 60\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "excel_file = 'results_df.xlsx'\n",
    "results_df.to_excel(excel_file, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {excel_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
