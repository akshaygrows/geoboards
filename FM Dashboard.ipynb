{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import psycopg2 as psy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conn(SSH_required,key_path):   #for getting a connection as a result\n",
    "\n",
    "    db='datawarehouse'\n",
    "    DB_HOST='datawarehouse.cdgpvetprks3.ap-south-1.rds.amazonaws.com'\n",
    "    conn = []\n",
    "    if SSH_required == 'Yes':\n",
    "        SSH_HOST='ec2-15-206-161-154.ap-south-1.compute.amazonaws.com'\n",
    "        #LOCALHOST=\"0.0.0.0\"\n",
    "        ssh_tunnel= SSHTunnelForwarder(\n",
    "                (SSH_HOST, 22),\n",
    "                ssh_username=\"ec2-user\",\n",
    "                ssh_private_key= key_path,\n",
    "                ssh_private_key_password= \"\",\n",
    "                remote_bind_address=(DB_HOST, 5432),\n",
    "                local_bind_address=('127.0.0.1', 0)\n",
    "        )\n",
    "        print('Tunnel Started')\n",
    "        ssh_tunnel.start()\n",
    "        conn = psy.connect(\n",
    "            host=ssh_tunnel.local_bind_host,\n",
    "            port=ssh_tunnel.local_bind_port,\n",
    "            user='postgres',\n",
    "            password= \"Simply1234\",\n",
    "            database='postgres')\n",
    "        print('Connection Made')\n",
    "        return conn\n",
    "    else:\n",
    "        conn = psy.connect(\n",
    "            host = DB_HOST,\n",
    "            port = 5432,\n",
    "            user = 'postgres',\n",
    "            password= \"Simply1234\",\n",
    "            database='postgres')\n",
    "        print('Connection Made')\n",
    "        return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_sql(SSH_required, query,key_path):   #for getting a datafarame as a result\n",
    "\n",
    "    db='datawarehouse'\n",
    "    DB_HOST='datawarehouse.cdgpvetprks3.ap-south-1.rds.amazonaws.com'\n",
    "    conn = None\n",
    "    if SSH_required == 'Yes':\n",
    "        SSH_HOST='ec2-15-206-161-154.ap-south-1.compute.amazonaws.com'\n",
    "        #LOCALHOST=\"0.0.0.0\"\n",
    "        ssh_tunnel= SSHTunnelForwarder(\n",
    "                (SSH_HOST, 22),\n",
    "                ssh_username=\"ec2-user\",\n",
    "                ssh_private_key= key_path,\n",
    "                ssh_private_key_password= \"\",\n",
    "                remote_bind_address=(DB_HOST, 5432),\n",
    "                local_bind_address=('127.0.0.1', 0)\n",
    "        )\n",
    "        # ssh_tunnel._server_list[0].block_on_close = False\n",
    "        ssh_tunnel.start()\n",
    "        conn = psy.connect(\n",
    "            host=ssh_tunnel.local_bind_host,\n",
    "            port=ssh_tunnel.local_bind_port,\n",
    "            user='postgres',\n",
    "            password= \"Simply1234\",\n",
    "            database='postgres')\n",
    "        df_results = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        ssh_tunnel.stop()\n",
    "        return df_results\n",
    "    else:\n",
    "        conn = psy.connect(\n",
    "            host = DB_HOST,\n",
    "            port = 5432,\n",
    "            user = 'postgres',\n",
    "            password= \"Simply1234\",\n",
    "            database='postgres')\n",
    "        df_results = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunnel Started\n",
      "Connection Made\n"
     ]
    }
   ],
   "source": [
    "# Usage with the actual path to the private key\n",
    "SSH_required = 'Yes'\n",
    "key_path = '/Users/rajatsansaniwal/Documents/tunnel-ssh .cer'\n",
    "query = \"\"\"with ops_main as (\n",
    "    select\n",
    "        awb\n",
    "    ,   pickuptime\n",
    "    from public.ops_main\n",
    "    where 1=1\n",
    "    and pickuptime is not null\n",
    "    and shipping_partner = 'Hyperlocal'\n",
    "    and date_trunc('day', pickuptime) = date_trunc('day', now() + interval'5.5 hours')\n",
    ")\n",
    ",\n",
    "shipment_order_details as (\n",
    "    select\n",
    "        awb\n",
    "    ,   warehouse_id\n",
    "    from public.shipment_order_details\n",
    ")\n",
    ",\n",
    "base as (\n",
    "    select\n",
    "        s.warehouse_id\n",
    "    ,   o.pickuptime\n",
    "    ,   o.awb\n",
    "    from ops_main o\n",
    "    left join shipment_order_details s on o.awb = s.awb\n",
    "    order by 1, 2\n",
    ")\n",
    "\n",
    "select * from base\"\"\"\n",
    "\n",
    "# Establish a connection\n",
    "conn = get_conn(SSH_required, key_path)\n",
    "\n",
    "# Retrieve data into a DataFrame\n",
    "df = get_df_from_sql(SSH_required, query, key_path)\n",
    "\n",
    "# Now you can perform further operations with the DataFrame 'df'\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 window  active  warehouse_id  pickup_start  pickup_end\n",
      "18  2024-07-05 00:00:00    True          1615          True       False\n",
      "278 2024-07-05 00:40:00   False          1615         False        True\n",
      "343 2024-07-05 00:50:00    True          1615          True       False\n",
      "408 2024-07-05 01:00:00   False          1615         False        True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 16:21:59,511| ERROR   | Socket exception: Operation timed out (60)\n"
     ]
    }
   ],
   "source": [
    "df['pickuptime'] = pd.to_datetime(df['pickuptime'])\n",
    "\n",
    "def round_down_to_nearest_10min(dt):\n",
    "    return dt - timedelta(minutes=dt.minute % 10, seconds=dt.second, microseconds=dt.microsecond)\n",
    "\n",
    "df['window'] = df['pickuptime'].apply(round_down_to_nearest_10min)\n",
    "\n",
    "# Generate all possible 10-minute windows\n",
    "start_time = df['pickuptime'].min().floor('10T')\n",
    "end_time = df['pickuptime'].max().ceil('10T')\n",
    "all_windows = pd.date_range(start=start_time, end=end_time, freq='10T')\n",
    "\n",
    "# Create a DataFrame for windows and initialize as inactive\n",
    "window_df = pd.DataFrame({'window': all_windows})\n",
    "window_df['active'] = False\n",
    "\n",
    "# Add warehouse_id to windows and initialize as inactive\n",
    "unique_warehouses = df['warehouse_id'].unique()\n",
    "window_df = window_df.merge(pd.DataFrame({'warehouse_id': unique_warehouses}), how='cross')\n",
    "window_df['active'] = False\n",
    "\n",
    "# Mark active windows based on pickups\n",
    "for warehouse in unique_warehouses:\n",
    "    warehouse_windows = df[df['warehouse_id'] == warehouse]['window'].unique()\n",
    "    window_df.loc[(window_df['warehouse_id'] == warehouse) & (window_df['window'].isin(warehouse_windows)), 'active'] = True\n",
    "\n",
    "# Identify pickup start and end times\n",
    "window_df['pickup_start'] = False\n",
    "window_df['pickup_end'] = False\n",
    "\n",
    "for warehouse in unique_warehouses:\n",
    "    warehouse_windows = window_df[window_df['warehouse_id'] == warehouse].reset_index(drop=True)\n",
    "    for i in range(len(warehouse_windows) - 1):\n",
    "        current_active = warehouse_windows.iloc[i]['active']\n",
    "        next_active = warehouse_windows.iloc[i + 1]['active']\n",
    "        if i == 0 and current_active:\n",
    "            window_df.loc[(window_df['warehouse_id'] == warehouse) & (window_df['window'] == warehouse_windows.iloc[i]['window']), 'pickup_start'] = True\n",
    "        if not current_active and next_active:\n",
    "            window_df.loc[(window_df['warehouse_id'] == warehouse) & (window_df['window'] == warehouse_windows.iloc[i + 1]['window']), 'pickup_start'] = True\n",
    "        if current_active and not next_active:\n",
    "            if i > 0:  # Ensure it's not the first window of the day\n",
    "                window_df.loc[(window_df['warehouse_id'] == warehouse) & (window_df['window'] == warehouse_windows.iloc[i + 1]['window']), 'pickup_end'] = True\n",
    "\n",
    "\n",
    "# Filter to show only start and end times\n",
    "pickup_times = window_df[(window_df['pickup_start']) | (window_df['pickup_end'])]\n",
    "\n",
    "# Display results\n",
    "print(pickup_times[pickup_times['warehouse_id'] == 1615])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     warehouse_id   pickup_start_time     pickup_end_time\n",
      "0             329 2024-07-05 10:50:00 2024-07-05 11:20:00\n",
      "1             370 2024-07-05 04:20:00 2024-07-05 04:30:00\n",
      "2             370 2024-07-05 10:50:00 2024-07-05 11:30:00\n",
      "3             370 2024-07-05 14:10:00 2024-07-05 14:20:00\n",
      "4             370 2024-07-05 15:00:00 2024-07-05 15:10:00\n",
      "..            ...                 ...                 ...\n",
      "117          2122 2024-07-05 13:50:00 2024-07-05 15:00:00\n",
      "118          2129 2024-07-05 11:50:00 2024-07-05 12:00:00\n",
      "119          2129 2024-07-05 14:10:00 2024-07-05 14:20:00\n",
      "120          2140 2024-07-05 11:10:00 2024-07-05 11:20:00\n",
      "121          2147 2024-07-05 11:50:00 2024-07-05 12:00:00\n",
      "\n",
      "[122 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Iterate through each warehouse_id\n",
    "for warehouse in window_df['warehouse_id'].unique():\n",
    "    # Filter rows for the current warehouse_id and reset index for iteration\n",
    "    warehouse_windows = window_df[window_df['warehouse_id'] == warehouse].reset_index(drop=True)\n",
    "    i = 0\n",
    "    while i < len(warehouse_windows):\n",
    "        if warehouse_windows.iloc[i]['pickup_start']:\n",
    "            # Find the corresponding pickup_end\n",
    "            j = i + 1\n",
    "            while j < len(warehouse_windows):\n",
    "                if warehouse_windows.iloc[j]['pickup_end']:\n",
    "                    # Append the result for this pair of pickup_start and pickup_end\n",
    "                    results.append({\n",
    "                        'warehouse_id': warehouse,\n",
    "                        'pickup_start_time': warehouse_windows.iloc[i]['window'],\n",
    "                        'pickup_end_time': warehouse_windows.iloc[j]['window']\n",
    "                    })\n",
    "                    i = j + 1  # Move to the next pickup_start\n",
    "                    break\n",
    "                j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "# Create DataFrame from results list\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the result\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
